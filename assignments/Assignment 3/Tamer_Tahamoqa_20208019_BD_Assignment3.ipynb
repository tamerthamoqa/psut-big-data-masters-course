{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stjZ7fFzVRDY"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1f0tvL6DLzfOQ_xHNdaBe_6fDUd27lc6U\" alt=\"PSUT\"  width=\"180px\"> \n",
    "<p> \n",
    "King Hussein School for Computing Sciences <br>\n",
    "Department of Data Science <br>\n",
    "\n",
    "<b>Big Data 2020-2021 </b> \n",
    "</p></center>\n",
    "\n",
    "\n",
    "<hr>\n",
    "<h4><b>Grading </b></h4>\n",
    "\n",
    "<table border>\n",
    "<tr>\n",
    "<td> <b>Question</b></td>\n",
    "<td> <b>Mark</b></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> <b>Q1</b></td>\n",
    "<td>  /1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> <b>Q2</b></td>\n",
    "<td>  /2</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> <b>Q3</b></td>\n",
    "<td>  /1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> <b>Q4</b></td>\n",
    "<td>  /2</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> <b>Q5</b></td>\n",
    "<td>  /1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <b>Q6</b></td>\n",
    "<td>  /2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <b>Q7</b></td>\n",
    "<td>  /1</td>\n",
    "</tr>\n",
    "<td> <b>Total</b></td>\n",
    "<th colspan=3> /10</th> \n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "<h4><b>Programming Assignment Guidelines</b></h4>\n",
    "<ul>\n",
    "\n",
    "\n",
    "<li>  The assignment should be performed individually.\n",
    "<li>  Cutoff date for this homeworkis 22/01/2021 @ 23:59\n",
    "<li>  Any late assignment will not be accepted.\n",
    "<li>  Academic Fraud: Cases of plagiarism will be dealt with according to university regulations.\n",
    "<li>  Your homework should be written in a jupyter notebook file\n",
    "<li>  Use Pyspark API in all questions and show all code in each step.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Name: Tamer Tahamoqa\n",
    "#### Student ID: 20208019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqt6ndX3iAPX"
   },
   "source": [
    "# Q1: Extract and Join Data \n",
    "\n",
    "Extract the data from the files (File1.csv) and (File2.json) as two Spark Dataframe and call them DF1 and DF2. \n",
    "\n",
    "  - How many rows and columns in DF1 and DF2? What are the names of columns in DF1 and DF2?\n",
    "\n",
    "  - Apply inner join on DF1 and DF2 on the ID column and name the new dataframe DF3. Calculate average,max,min, and median for each numerical column in DF3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q1.1 How many rows and columns in DF1 and DF2? What are the names of columns in DF1 and DF2?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Reading csv file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = spark.read.csv(\"File1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F1: string (nullable = true)\n",
      " |-- F2: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+\n",
      "|        F1|                  F2| ID|\n",
      "+----------+--------------------+---+\n",
      "|     Truck|0.012685463735511316|  0|\n",
      "|       Car|0.003089755131087...|  1|\n",
      "|Train_type|0.029100348249081864|  2|\n",
      "|       Car|0.006820733183862894|  3|\n",
      "|  Airplane|-0.02160027347051...|  4|\n",
      "|  Airplane|0.002929568716950...|  5|\n",
      "|  Airplane| 0.02066010054953399|  6|\n",
      "|     Train|0.007345002241446708|  7|\n",
      "|     Train|0.005374476001636689|  8|\n",
      "|     Train|0.021318944757115293|  9|\n",
      "+----------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Answer for DF1 (csv file):__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+-----------------+\n",
      "|summary|        F1|                  F2|               ID|\n",
      "+-------+----------+--------------------+-----------------+\n",
      "|  count|      1068|                1061|             1090|\n",
      "|   mean|      null|0.003554136882456...|            544.5|\n",
      "| stddev|      null|0.013737258998262994|314.8002011858739|\n",
      "|    min|  Airplane|-0.04080206415504...|                0|\n",
      "|    25%|      null|-0.00586363990023...|              272|\n",
      "|    50%|      null|0.003571329130768...|              544|\n",
      "|    75%|      null| 0.01260794151844527|              817|\n",
      "|    max|Truck_type| 0.04469887046022417|             1089|\n",
      "+-------+----------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Reading json file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = spark.read.format(\"json\").load(\"File2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  F3|                  F4|                  F5|                  ID|               Label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[19.6516957132, 2...|[-4.8951415031, -...|[Green, Yellow, Y...|[0, 1, 10, 100, 1...|[-0.253787957, -1...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F3', 'F4', 'F5', 'ID', 'Label']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackOverflow post reference for reading nested JSON files: https://stackoverflow.com/a/57812630\n",
    "DF2 = DF2.select(\n",
    "    F.array(F.expr(\"F3.*\")).alias(\"F3\"),\n",
    "    F.array(F.expr(\"F4.*\")).alias(\"F4\"),\n",
    "    F.array(F.expr(\"F5.*\")).alias(\"F5\"),\n",
    "    F.array(F.expr(\"ID.*\")).alias(\"ID\"),\n",
    "    F.array(F.expr(\"Label.*\")).alias(\"Label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F3: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- F4: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- F5: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ID: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Label: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  F3|                  F4|                  F5|                  ID|               Label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[19.6516957132, 2...|[-4.8951415031, -...|[Green, Yellow, Y...|[0, 1, 10, 100, 1...|[-0.253787957, -1...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackOverflow post reference for reading nested JSON files: https://stackoverflow.com/a/57812630\n",
    "DF2 = DF2.withColumn(\"Row\", F.explode(F.arrays_zip(\"F3\", \"F4\", \"F5\", \"ID\", \"Label\"))) \\\n",
    "    .select(\"Row.F3\", \"Row.F4\", \"Row.F5\", \"Row.ID\", \"Row.Label\").sort(\"Row.ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F3: string (nullable = true)\n",
      " |-- F4: string (nullable = true)\n",
      " |-- F5: string (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+---+-------------+\n",
      "|           F3|           F4|         F5| ID|        Label|\n",
      "+-------------+-------------+-----------+---+-------------+\n",
      "|19.6516957132|-4.8951415031|      Green|  0| -0.253787957|\n",
      "|23.1331849578|-5.1369714071|     Yellow|  1|-1.4292600841|\n",
      "|19.5058194483|-4.9084296303|        Red|  2| 1.8115613359|\n",
      "|21.3939123613|-4.8980267453|      Black|  3|-0.4364901966|\n",
      "|18.2535890658|-5.1502555307|        Red|  4|-0.9192026185|\n",
      "|18.6295203092|         null|Green_color|  5| 0.4519735606|\n",
      "|18.1389196653|-4.8998850898|      Green|  6| 2.9719232276|\n",
      "|18.1907797287|-4.7535255261|      Green|  7| 0.5901298058|\n",
      "|18.8943618047|-4.8434685167|      Green|  8| 1.3846172152|\n",
      "|19.0654998844|-5.1191803913|      Green|  9| 0.5630485316|\n",
      "+-------------+-------------+-----------+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Answer for DF2 (json file):__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------+-----------------+-------------------+\n",
      "|summary|                F3|                 F4|          F5|               ID|              Label|\n",
      "+-------+------------------+-------------------+------------+-----------------+-------------------+\n",
      "|  count|              1061|               1071|        1090|             1090|               1090|\n",
      "|   mean|20.038100913703207| -5.001615744943602|        null|            544.5|0.08961207953504582|\n",
      "| stddev|1.7714335084999626|0.17148835054089803|        null|314.8002011858739| 1.4258820337976077|\n",
      "|    min|     13.8336591166|      -4.4871907798|       Black|                0|      -3.9166166984|\n",
      "|    25%|     18.7976010734|      -5.1261509311|        null|              272|      -0.8517109068|\n",
      "|    50%|     20.0608982698|      -4.9998842161|        null|              544|       0.0738270339|\n",
      "|    75%|     21.2773559332|      -4.8755903181|        null|              817|       1.1013826045|\n",
      "|    max|      25.429990436|      -5.4860823663|Yellow_color|             1089|       4.9927369335|\n",
      "+-------+------------------+-------------------+------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q1.2 Apply inner join on DF1 and DF2 on the ID column and name the new dataframe DF3. Calculate average,max,min, and median for each numerical column in DF3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF3 = DF1.join(DF2, on=\"ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- F1: string (nullable = true)\n",
      " |-- F2: double (nullable = true)\n",
      " |-- F3: string (nullable = true)\n",
      " |-- F4: string (nullable = true)\n",
      " |-- F5: string (nullable = true)\n",
      " |-- Label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change F3 and F4 columns to Double data type\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "DF3 = DF3.withColumn(\"F3\", DF3[\"F3\"].cast(DoubleType()))\n",
    "DF3 = DF3.withColumn(\"F4\", DF3[\"F4\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- F1: string (nullable = true)\n",
      " |-- F2: double (nullable = true)\n",
      " |-- F3: double (nullable = true)\n",
      " |-- F4: double (nullable = true)\n",
      " |-- F5: string (nullable = true)\n",
      " |-- Label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+-------------+-------------+-----------+-------------+\n",
      "| ID|        F1|                  F2|           F3|           F4|         F5|        Label|\n",
      "+---+----------+--------------------+-------------+-------------+-----------+-------------+\n",
      "|  0|     Truck|0.012685463735511316|19.6516957132|-4.8951415031|      Green| -0.253787957|\n",
      "|  1|       Car|0.003089755131087...|23.1331849578|-5.1369714071|     Yellow|-1.4292600841|\n",
      "|  2|Train_type|0.029100348249081864|19.5058194483|-4.9084296303|        Red| 1.8115613359|\n",
      "|  3|       Car|0.006820733183862894|21.3939123613|-4.8980267453|      Black|-0.4364901966|\n",
      "|  4|  Airplane|-0.02160027347051...|18.2535890658|-5.1502555307|        Red|-0.9192026185|\n",
      "|  5|  Airplane|0.002929568716950...|18.6295203092|         null|Green_color| 0.4519735606|\n",
      "|  6|  Airplane| 0.02066010054953399|18.1389196653|-4.8998850898|      Green| 2.9719232276|\n",
      "|  7|     Train|0.007345002241446708|18.1907797287|-4.7535255261|      Green| 0.5901298058|\n",
      "|  8|     Train|0.005374476001636689|18.8943618047|-4.8434685167|      Green| 1.3846172152|\n",
      "|  9|     Train|0.021318944757115293|19.0654998844|-5.1191803913|      Green| 0.5630485316|\n",
      "+---+----------+--------------------+-------------+-------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q1.2 Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\"F2\", \"F3\", \"F4\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+--------------------+------------------+-------------------+------------+-------------------+\n",
      "|summary|               ID|        F1|                  F2|                F3|                 F4|          F5|              Label|\n",
      "+-------+-----------------+----------+--------------------+------------------+-------------------+------------+-------------------+\n",
      "|  count|             1090|      1068|                1061|              1061|               1071|        1090|               1090|\n",
      "|   mean|            544.5|      null|0.003554136882456...|20.038100913703207|   -5.0016157449436|        null|0.08961207953504591|\n",
      "| stddev|314.8002011858739|      null|0.013737258998262994|1.7714335084999617|0.17148835054089792|        null|  1.425882033797608|\n",
      "|    min|                0|  Airplane|-0.04080206415504...|     13.8336591166|      -5.4860823663|       Black|      -3.9166166984|\n",
      "|    25%|              272|      null|-0.00586363990023...|     18.7976010734|      -5.1261509311|        null|      -0.8517109068|\n",
      "|    50%|              544|      null|0.003571329130768...|     20.0608982698|      -4.9998842161|        null|       0.0738270339|\n",
      "|    75%|              817|      null| 0.01260794151844527|     21.2773559332|      -4.8755903181|        null|       1.1013826045|\n",
      "|    max|             1089|Truck_type| 0.04469887046022417|      25.429990436|      -4.4871907798|Yellow_color|       4.9927369335|\n",
      "+-------+-----------------+----------+--------------------+------------------+-------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F2', 'F3', 'F4', 'Label']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0035713291307688955], [20.0608982698], [-4.9998842161], [0.0738270339]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating median values for numerical columns\n",
    "print(numerical_columns)\n",
    "DF3.approxQuantile(numerical_columns, probabilities=[0.5], relativeError=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMIMRoPziOYq"
   },
   "source": [
    "# Q2: Dealing with missing values \n",
    "\n",
    "   - How many missing values in each column in DF3? \n",
    "   - What is the percentage of missing value in each column in DF3?\n",
    "   - Impute missing values in the numerical columns by its median. \n",
    "   - Impute missing values the categorical columns by its mode.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q2.1 How many missing values in each column in DF3?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+-----+\n",
      "| ID| F1| F2| F3| F4| F5|Label|\n",
      "+---+---+---+---+---+---+-----+\n",
      "|  0|  0|  0|  0|  0|  0|    0|\n",
      "+---+---+---+---+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting number of missing (nan) values in each column, used this blogpost as a reference:\n",
    "#  https://www.datasciencemadesimple.com/count-of-missing-nanna-and-null-values-in-pyspark/\n",
    "DF3.select([F.count(F.when(F.isnan(col), col)).alias(col) for col in DF3.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q2.1 Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+-----+\n",
      "| ID| F1| F2| F3| F4| F5|Label|\n",
      "+---+---+---+---+---+---+-----+\n",
      "|  0| 22| 29| 29| 19|  0|    0|\n",
      "+---+---+---+---+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting number of null values in each column, used this blogpost as a reference:\n",
    "#  https://www.datasciencemadesimple.com/count-of-missing-nanna-and-null-values-in-pyspark/\n",
    "DF3.select([F.count(F.when(F.isnull(col), col)).alias(col) for col in DF3.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q2.2 What is the percentage of missing value in each column in DF3?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-------+-------+---+-----+\n",
      "| ID|    F1|     F2|     F3|     F4| F5|Label|\n",
      "+---+------+-------+-------+-------+---+-----+\n",
      "|0.0|0.0206|0.02733|0.02733|0.01774|0.0|  0.0|\n",
      "+---+------+-------+-------+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.select(\n",
    "    [F.round(\n",
    "        F.count(F.when(F.isnull(col), col)) / F.count(col),\n",
    "        scale=5 # 5 digits\n",
    "    ).alias(col) \\\n",
    "    for col in DF3.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q2.3 Impute missing values in the numerical columns by its median.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical_columns_that_contain_nulls = [\"F2\", \"F3\", \"F4\"]\n",
    "DF3 = DF3.na.fill(\n",
    "    {\n",
    "        \"F2\": DF3.approxQuantile(\"F2\", probabilities=[0.5], relativeError=0)[0],\n",
    "        \"F3\": DF3.approxQuantile(\"F3\", probabilities=[0.5], relativeError=0)[0],\n",
    "        \"F4\": DF3.approxQuantile(\"F4\", probabilities=[0.5], relativeError=0)[0]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   __Q2.4 Impute missing values the categorical columns by its mode.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'F1', 'F2', 'F3', 'F4', 'F5', 'Label']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[85,\n",
       " 'Train',\n",
       " 0.0035713291307688955,\n",
       " 20.0608982698,\n",
       " -4.9998842161,\n",
       " 'Black',\n",
       " 1.2022263408]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing calculating the mode value for all columns\n",
    "#  StackOverflow post: https://stackoverflow.com/a/58279672\n",
    "print(DF3.columns)\n",
    "[DF3.groupby(col).count().orderBy(\"count\", ascending=False).first()[0] for col in DF3.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical_columns_that_contain_nulls = [\"F1\", \"F5\"]\n",
    "#  StackOverflow post: https://stackoverflow.com/a/58279672\n",
    "DF3 = DF3.na.fill(\n",
    "    {\n",
    "        \"F1\": DF3.groupby(\"F1\").count().orderBy(\"count\", ascending=False).first()[0],\n",
    "        \"F5\": DF3.groupby(\"F5\").count().orderBy(\"count\", ascending=False).first()[0]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Checking if there are null values after imputation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+-----+\n",
      "| ID| F1| F2| F3| F4| F5|Label|\n",
      "+---+---+---+---+---+---+-----+\n",
      "|  0|  0|  0|  0|  0|  0|    0|\n",
      "+---+---+---+---+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF3.select([F.count(F.when(F.isnull(col), col)).alias(col) for col in DF3.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEBnD2tAiTHw"
   },
   "source": [
    "# Q3: Dealing with duplicated rows without the ID column.\n",
    "   \n",
    "   - Create DF4 by droping the ID column from DF3.\n",
    "   - How many duplicated rows in DF4?\n",
    "   - Create DF5 after dropping all duplicated rows in DF4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q3.1 Create DF4 by dropping the ID column from DF3.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF4 = DF3.drop(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q3.2 How many duplicated rows in DF4?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       148|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking number of duplicate rows\n",
    "#  Used this stackoverflow answer: https://stackoverflow.com/a/48554666\n",
    "DF4.groupBy(DF4.columns) \\\n",
    "    .count() \\\n",
    "    .where(F.col('count') > 1) \\\n",
    "    .select(F.sum('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q3.3 Create DF5 after dropping all duplicated rows in DF4.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF5 = DF4.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------------------+-------------------+------------+------------------+\n",
      "|summary|        F1|                  F2|                F3|                 F4|          F5|             Label|\n",
      "+-------+----------+--------------------+------------------+-------------------+------------+------------------+\n",
      "|  count|      1016|                1016|              1016|               1016|        1016|              1016|\n",
      "|   mean|      null|0.003600390942043...|20.050995009598527| -5.002929277792814|        null| 0.104013753299311|\n",
      "| stddev|      null|0.013473749804847168|1.7564082060232336|0.17073042947311726|        null|1.4430335407697776|\n",
      "|    min|  Airplane|-0.04080206415504...|     13.8336591166|      -5.4860823663|       Black|     -3.9166166984|\n",
      "|    25%|      null|-0.00552332371640...|      18.829667492|      -5.1228632953|        null|     -0.8454540831|\n",
      "|    50%|      null|0.003571329130768...|     20.0608982698|      -4.9998842161|        null|      0.0931091041|\n",
      "|    75%|      null|0.012338382001062081|     21.2695972677|      -4.8818324173|        null|      1.1049338284|\n",
      "|    max|Truck_type| 0.04469887046022417|      25.429990436|      -4.4871907798|Yellow_color|      4.9927369335|\n",
      "+-------+----------+--------------------+------------------+-------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF5.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLlf8b_5iZRI"
   },
   "source": [
    "# Q4: Dealing with inconsistancy in categorial colums \n",
    "\n",
    "   - Identify columns that has inconsistent categorical values?\n",
    "   - Provide a  transformation to fix the inconsistancy.\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q4.1 Identify columns that has inconsistent categorical values?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           F1|\n",
      "+-------------+\n",
      "|     Airplane|\n",
      "|   Train_type|\n",
      "|          Car|\n",
      "|        Truck|\n",
      "|Airplane_type|\n",
      "|   Truck_type|\n",
      "|        Train|\n",
      "|     Car_type|\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|          F5|\n",
      "+------------+\n",
      "| Black_color|\n",
      "|Yellow_color|\n",
      "| Green_color|\n",
      "| White_color|\n",
      "|   Red_color|\n",
      "|       Green|\n",
      "|       White|\n",
      "|      Yellow|\n",
      "|       Black|\n",
      "|         Red|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Categorical_columns = [\"F1\", \"F5\"]\n",
    "DF5.select(\"F1\").distinct().show()\n",
    "DF5.select(\"F5\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q4.1 Answer:__\n",
    "Both categorical columns have inconsistent values. Column F1 has values that have __\"_type\"__ appended and some do not. Column F5 has values that have __\"_color\"__ appended and some do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q4.2 Provide a  transformation to fix the inconsistancy.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __4.2.1 Removing \"_type\" from column F1 values by replacing the string pattern \"_type\" with empty string__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackOverflow post: https://stackoverflow.com/a/37038231\n",
    "DF5 = DF5.withColumn(\"F1\", F.regexp_replace(\"F1\", pattern=\"_type\", replacement=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|      F1|\n",
      "+--------+\n",
      "|Airplane|\n",
      "|     Car|\n",
      "|   Truck|\n",
      "|   Train|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF5.select(\"F1\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __4.2.1 Removing \"_color\" from column F5 values by replacing the string pattern \"_color\" with empty string__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackOverflow post: https://stackoverflow.com/a/37038231\n",
    "DF5 = DF5.withColumn(\"F5\", F.regexp_replace(\"F5\", pattern=\"_color\", replacement=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    F5|\n",
      "+------+\n",
      "| Green|\n",
      "| White|\n",
      "|Yellow|\n",
      "| Black|\n",
      "|   Red|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF5.select(\"F5\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_1pINYWir6S"
   },
   "source": [
    "# Q5: Dealing with categorical columns:\n",
    "\n",
    "- Propose a transformation to convert the categorical columns in DF5 into numerical columns.\n",
    "- Create a dataframe DF6 that has the new transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q5.1 Propose a transformation to convert the categorical columns in DF5 into numerical columns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q5.1 Answer__:\n",
    "By encoding the categorical features using StringIndexer and then passing the encoding features into a OneHotEncoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q5.2 Create a dataframe DF6 that has the new transformations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categorical_columns = [\"F1\", \"F5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\")\n",
    "    for col in categorical_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoders = [\n",
    "    OneHotEncoder(inputCol=f'{col}_indexed', outputCol=f'{col}_onehotencoded', dropLast=True)  # for k-1 columns\n",
    "    for col in categorical_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=string_indexers + onehot_encoders)\n",
    "pipeline_model = pipeline.fit(DF5)\n",
    "\n",
    "DF6 = pipeline_model.transform(DF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F1: string (nullable = false)\n",
      " |-- F2: double (nullable = false)\n",
      " |-- F3: double (nullable = false)\n",
      " |-- F4: double (nullable = false)\n",
      " |-- F5: string (nullable = false)\n",
      " |-- Label: double (nullable = true)\n",
      " |-- F1_indexed: double (nullable = false)\n",
      " |-- F5_indexed: double (nullable = false)\n",
      " |-- F1_onehotencoded: vector (nullable = true)\n",
      " |-- F5_onehotencoded: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+-------------+------+-------------+----------+----------+----------------+----------------+\n",
      "|      F1|                  F2|           F3|           F4|    F5|        Label|F1_indexed|F5_indexed|F1_onehotencoded|F5_onehotencoded|\n",
      "+--------+--------------------+-------------+-------------+------+-------------+----------+----------+----------------+----------------+\n",
      "|   Train|0.002757799599936316|19.3168642328|-4.9341584372|   Red|-0.8517109068|       0.0|       1.0|   (3,[0],[1.0])|   (4,[1],[1.0])|\n",
      "|   Train| 0.01522005357438317|18.4782122435|-4.9615511543| Green| 0.7645562902|       0.0|       4.0|   (3,[0],[1.0])|       (4,[],[])|\n",
      "|     Car|0.004548906412360894|22.0379592508|-4.7621026096| White|-0.1682166727|       2.0|       3.0|   (3,[2],[1.0])|   (4,[3],[1.0])|\n",
      "|   Truck|-0.00385605869301...|18.4353543385|-4.8272261658|Yellow|-0.1504823794|       1.0|       2.0|   (3,[1],[1.0])|   (4,[2],[1.0])|\n",
      "|Airplane|0.003571329130768...|17.5574672255|-4.8817233869| Black| 2.5008399099|       3.0|       0.0|       (3,[],[])|   (4,[0],[1.0])|\n",
      "|Airplane|6.828809833279298E-5|18.1194063041|-4.9979315919| Green| 0.3703458017|       3.0|       4.0|       (3,[],[])|       (4,[],[])|\n",
      "|   Train|-0.00352404178627...|21.9858593648|-5.2465342453| Green|-1.1937191357|       0.0|       4.0|   (3,[0],[1.0])|       (4,[],[])|\n",
      "|Airplane|-0.01141675662638...|18.4922112532|-5.2578346065|   Red|-0.6283674193|       3.0|       1.0|       (3,[],[])|   (4,[1],[1.0])|\n",
      "|     Car|0.008917237528243142|18.8611966392|-4.9764984162| White|-0.1172424373|       2.0|       3.0|   (3,[2],[1.0])|   (4,[3],[1.0])|\n",
      "|Airplane|-0.00750894447695...|19.0363827494| -5.306937549|   Red|-0.7615600037|       3.0|       1.0|       (3,[],[])|   (4,[1],[1.0])|\n",
      "+--------+--------------------+-------------+-------------+------+-------------+----------+----------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF6 = DF6.drop(\"F1\", \"F5\", \"F1_indexed\", \"F5_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F2: double (nullable = false)\n",
      " |-- F3: double (nullable = false)\n",
      " |-- F4: double (nullable = false)\n",
      " |-- Label: double (nullable = true)\n",
      " |-- F1_onehotencoded: vector (nullable = true)\n",
      " |-- F5_onehotencoded: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+-------------+----------------+----------------+\n",
      "|                  F2|           F3|           F4|        Label|F1_onehotencoded|F5_onehotencoded|\n",
      "+--------------------+-------------+-------------+-------------+----------------+----------------+\n",
      "|0.002757799599936316|19.3168642328|-4.9341584372|-0.8517109068|   (3,[0],[1.0])|   (4,[1],[1.0])|\n",
      "| 0.01522005357438317|18.4782122435|-4.9615511543| 0.7645562902|   (3,[0],[1.0])|       (4,[],[])|\n",
      "|0.004548906412360894|22.0379592508|-4.7621026096|-0.1682166727|   (3,[2],[1.0])|   (4,[3],[1.0])|\n",
      "|-0.00385605869301...|18.4353543385|-4.8272261658|-0.1504823794|   (3,[1],[1.0])|   (4,[2],[1.0])|\n",
      "|0.003571329130768...|17.5574672255|-4.8817233869| 2.5008399099|       (3,[],[])|   (4,[0],[1.0])|\n",
      "|6.828809833279298E-5|18.1194063041|-4.9979315919| 0.3703458017|       (3,[],[])|       (4,[],[])|\n",
      "|-0.00352404178627...|21.9858593648|-5.2465342453|-1.1937191357|   (3,[0],[1.0])|       (4,[],[])|\n",
      "|-0.01141675662638...|18.4922112532|-5.2578346065|-0.6283674193|       (3,[],[])|   (4,[1],[1.0])|\n",
      "|0.008917237528243142|18.8611966392|-4.9764984162|-0.1172424373|   (3,[2],[1.0])|   (4,[3],[1.0])|\n",
      "|-0.00750894447695...|19.0363827494| -5.306937549|-0.7615600037|       (3,[],[])|   (4,[1],[1.0])|\n",
      "+--------------------+-------------+-------------+-------------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Creating 'features' vector column for later steps__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = [\"F1_onehotencoded\", \"F2\", \"F3\", \"F4\", \"F5_onehotencoded\"]\n",
    "\n",
    "vector_assemblers = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector_assemblers])\n",
    "pipeline_model = pipeline.fit(DF6)\n",
    "\n",
    "DF6_vectorized = pipeline_model.transform(DF6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F2: double (nullable = false)\n",
      " |-- F3: double (nullable = false)\n",
      " |-- F4: double (nullable = false)\n",
      " |-- Label: double (nullable = true)\n",
      " |-- F1_onehotencoded: vector (nullable = true)\n",
      " |-- F5_onehotencoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6_vectorized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF6_vectorized = DF6_vectorized.drop(*feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF6_vectorized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[...|\n",
      "| 0.7645562902|(10,[0,3,4,5],[1....|\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[...|\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[...|\n",
      "| 2.5008399099|(10,[3,4,5,6],[0....|\n",
      "| 0.3703458017|(10,[3,4,5],[6.82...|\n",
      "|-1.1937191357|(10,[0,3,4,5],[1....|\n",
      "|-0.6283674193|(10,[3,4,5,7],[-0...|\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[...|\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF6_vectorized.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03oGdN1Wi2dd"
   },
   "source": [
    "# Q6: Linear Regression Model \n",
    "- Propose a transformation to split the Dataframe DF6 into into training and testing splits. Use X_train, y_train to store the features columns and label column for the training part, respectively, and use X_test,y_test to store the features columns and label column for the testig part, respectively. Use 70% of the rows for training and 30% of rows for testing.\n",
    "\n",
    "- Instantiate and train a Regression models (Linear regression or Decision Tree regression using) on  X_train, y_train.\n",
    "- Generate y_predict by passing X_test to the model\n",
    "- Calculate mean_squared_error or MSE and R2 using y_test and y_predict?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q6.1 Propose a transformation to split the Dataframe DF6 into into training and testing splits. Use X_train, y_train to store the features columns and label column for the training part, respectively, and use X_test,y_test to store the features columns and label column for the testing part, respectively. Use 70% of the rows for training and 30% of rows for testing.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __NOTE: I haven't split the data into X_train, y_train, X_test, y_test since PySpark ML model .fit() expects a full pyspark dataframe as input so I split into train_df, test_df__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a value for random seed value for reproducible train/test splits\n",
    "train_df, test_df = DF6_vectorized.randomSplit(weights=[0.7, 0.3], seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                745|\n",
      "|   mean|0.11241300524214759|\n",
      "| stddev| 1.4118029859923613|\n",
      "|    min|       -3.820947183|\n",
      "|    25%|      -0.8517109068|\n",
      "|    50%|       0.0931091041|\n",
      "|    75%|       1.1082230684|\n",
      "|    max|       4.4428105982|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                271|\n",
      "|   mean|0.08092355884391142|\n",
      "| stddev| 1.5280734856793479|\n",
      "|    min|      -3.9166166984|\n",
      "|    25%|      -0.8415096365|\n",
      "|    50%|       0.0991737429|\n",
      "|    75%|        1.052074211|\n",
      "|    max|       4.9927369335|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.summary().show()\n",
    "test_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[...|\n",
      "| 0.7645562902|(10,[0,3,4,5],[1....|\n",
      "|-1.1937191357|(10,[0,3,4,5],[1....|\n",
      "|-0.6283674193|(10,[3,4,5,7],[-0...|\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[...|\n",
      "| 0.3703458017|(10,[3,4,5],[6.82...|\n",
      "| 1.0793990547|(10,[0,3,4,5,8],[...|\n",
      "| 1.5540425054|(10,[3,4,5,9],[-0...|\n",
      "| -3.820947183|(10,[2,3,4,5,8],[...|\n",
      "|-0.7602633557|(10,[2,3,4,5],[1....|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0...|\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[...|\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[...|\n",
      "| 0.2151428872|(10,[2,3,4,5],[1....|\n",
      "| 1.5208218686|(10,[0,3,4,5,6],[...|\n",
      "| 2.4562802894|(10,[0,3,4,5,6],[...|\n",
      "| 2.5008399099|(10,[3,4,5,6],[0....|\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[...|\n",
      "| 0.6525371034|(10,[3,4,5,7],[-0...|\n",
      "| 2.6558999672|(10,[2,3,4,5,9],[...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(10)\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q6.2 Instantiate and train a Regression model (Linear regression or Decision Tree regression using) on  X_train, y_train.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Decision Tree Regression model\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='Label',\n",
    "    predictionCol='prediction',\n",
    "    maxDepth=5,\n",
    "    maxBins=32,\n",
    "    minInstancesPerNode=1,\n",
    "    impurity='variance',\n",
    "    seed=420  # For reproducible results\n",
    ")\n",
    "\n",
    "model = model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='labelCol', doc='label column name.'): 'Label',\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='seed', doc='random seed.'): 420,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='DecisionTreeRegressor_32d0ce528dfe', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q6.3 Generate y_predict by passing X_test to the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------------------+\n",
      "|        Label|            features|         prediction|\n",
      "+-------------+--------------------+-------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0...|-0.6254486540543479|\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[...| 1.0526800411315786|\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[...| 1.0526800411315786|\n",
      "| 0.2151428872|(10,[2,3,4,5],[1....|-0.5579046679268292|\n",
      "| 1.5208218686|(10,[0,3,4,5,6],[...| 0.5759052869290322|\n",
      "| 2.4562802894|(10,[0,3,4,5,6],[...| 0.5759052869290322|\n",
      "| 2.5008399099|(10,[3,4,5,6],[0....| 0.5759052869290322|\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[...| 0.2270330801291666|\n",
      "| 0.6525371034|(10,[3,4,5,7],[-0...| -0.564291289317647|\n",
      "| 2.6558999672|(10,[2,3,4,5,9],[...|  2.007974336257143|\n",
      "+-------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   __Q6.4 Calculate mean_squared_error or MSE and R2 using y_test and y_predict?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Mean Squared Error) = 1.1178622978396393\n",
      "r2 (R-Squared metric (coefficient of determination)) = 0.5194867636382727\n"
     ]
    }
   ],
   "source": [
    "mse = evaluator_mse.evaluate(y_predict)\n",
    "r2 = evaluator_r2.evaluate(y_predict)\n",
    "\n",
    "print(f\"MSE (Mean Squared Error) = {mse}\")\n",
    "print(f\"r2 (R-Squared metric (coefficient of determination)) = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp-IG61j14rl"
   },
   "source": [
    "# Q7: Scaling and normalization \n",
    "\n",
    "- Use  StandardScaler to normalize the featured in DF6 and split the DF6 into 30% testing and 70% training, then build linear regression model and calculate  R2 and MSE.\n",
    "\n",
    "- Use from MinMaxScaler to scale the featured in DF6 and split the DF6 into 30% testing and 70% training, then build linear regression model and calculate R2 and MSE.\n",
    "\n",
    "\n",
    "- Based on R2 and MSE, which model perform better? Justify your answer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_to_scale = [\"F2\", \"F3\", \"F4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q7.1 Use  StandardScaler to normalize the features in DF6 and split the DF6 into 30% testing and 70% training, then build linear regression model and calculate  R2 and MSE.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Scaler requires the column be in vector format\n",
    "#  https://stackoverflow.com/a/60281624\n",
    "vector_assemblers = [\n",
    "    VectorAssembler(inputCols=[column], outputCol=column + \"_vector\")\n",
    "    for column in numerical_columns_to_scale\n",
    "]\n",
    "\n",
    "standard_scalers = [\n",
    "    # Standardize to have unit standard deviation and zero mean\n",
    "    StandardScaler(inputCol=column + \"_vector\", outputCol=column + \"_scaled\", withStd=True, withMean=True)\n",
    "    for column in numerical_columns_to_scale\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=vector_assemblers + standard_scalers)\n",
    "scaler_model = pipeline.fit(DF6)\n",
    "\n",
    "standard_scaled_DF6 = scaler_model.transform(DF6)\n",
    "\n",
    "columns_to_drop = [col for col in numerical_columns_to_scale] + [f'{col}_vector' for col in numerical_columns_to_scale]\n",
    "standard_scaled_DF6 = standard_scaled_DF6.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "|        Label|F1_onehotencoded|F5_onehotencoded|           F2_scaled|           F3_scaled|           F4_scaled|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "|-0.8517109068|   (3,[0],[1.0])|   (4,[1],[1.0])|[-0.0625357717273...|[-0.4179727550127...|[0.4028036525477429]|\n",
      "| 0.7645562902|   (3,[0],[1.0])|       (4,[],[])|[0.8623926375832908]|[-0.8954540070497...|[0.24235939440034...|\n",
      "|-0.1682166727|   (3,[2],[1.0])|   (4,[3],[1.0])|[0.07039728984547...| [1.131265633118539]|[1.4105667568225488]|\n",
      "|-0.1504823794|   (3,[1],[1.0])|   (4,[2],[1.0])|[-0.5534056771917...|[-0.9198548865565...|[1.0291259298945417]|\n",
      "| 2.5008399099|       (3,[],[])|   (4,[0],[1.0])|[-0.0021569208049...|[-1.419674410280878]|[0.7099255315344997]|\n",
      "| 0.3703458017|       (3,[],[])|       (4,[],[])|[-0.2621469817140...|[-1.099737919052389]|[0.02927237931890...|\n",
      "|-1.1937191357|   (3,[0],[1.0])|       (4,[],[])|[-0.5287639173584...|[1.1016028896735157]|[-1.426839774602344]|\n",
      "|-0.6283674193|       (3,[],[])|   (4,[1],[1.0])|[-1.1145484951060...|[-0.8874837586462...|[-1.4930280998755...|\n",
      "|-0.1172424373|   (3,[2],[1.0])|   (4,[3],[1.0])|[0.39460778648915...|[-0.6774042425435...|[0.15481049086787...|\n",
      "|-0.7615600037|       (3,[],[])|   (4,[1],[1.0])|[-0.8245169741090...|[-0.5776631290602...|[-1.7806332013886...|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_scaled_DF6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Creating 'features' column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------------------------------------------------------+\n",
      "|Label        |features                                                                              |\n",
      "+-------------+--------------------------------------------------------------------------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[1.0,-0.06253577172735021,-0.4179727550127485,0.4028036525477429,1.0])|\n",
      "|0.7645562902 |(10,[0,3,4,5],[1.0,0.8623926375832908,-0.8954540070497292,0.24235939440034718])       |\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[1.0,0.07039728984547404,1.131265633118539,1.4105667568225488,1.0])   |\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[1.0,-0.5534056771917291,-0.9198548865565694,1.0291259298945417,1.0]) |\n",
      "|2.5008399099 |(10,[3,4,5,6],[-0.002156920804950496,-1.419674410280878,0.7099255315344997,1.0])      |\n",
      "|0.3703458017 |(10,[3,4,5],[-0.26214698171404394,-1.099737919052389,0.029272379318905212])           |\n",
      "|-1.1937191357|(10,[0,3,4,5],[1.0,-0.5287639173584145,1.1016028896735157,-1.426839774602344])        |\n",
      "|-0.6283674193|(10,[3,4,5,7],[-1.1145484951060318,-0.8874837586462009,-1.4930280998755452,1.0])      |\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[1.0,0.39460778648915845,-0.6774042425435998,0.15481049086787357,1.0])|\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0.8245169741090795,-0.5776631290602773,-1.7806332013886939,1.0])      |\n",
      "+-------------+--------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\"F1_onehotencoded\", \"F2_scaled\", \"F3_scaled\", \"F4_scaled\", \"F5_onehotencoded\"]\n",
    "\n",
    "vector_assemblers = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector_assemblers])\n",
    "pipeline_model = pipeline.fit(standard_scaled_DF6)\n",
    "\n",
    "standard_scaled_DF6_vectorized = pipeline_model.transform(standard_scaled_DF6)\n",
    "standard_scaled_DF6_vectorized = standard_scaled_DF6_vectorized.drop(*feature_columns)\n",
    "\n",
    "standard_scaled_DF6_vectorized.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Splitting and fitting to LinearRegression model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a value for random seed value for reproducible train/test splits\n",
    "train_df_standard, test_df_standard = standard_scaled_DF6_vectorized.randomSplit(weights=[0.7, 0.3], seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                745|\n",
      "|   mean|0.11241300524214759|\n",
      "| stddev| 1.4118029859923613|\n",
      "|    min|       -3.820947183|\n",
      "|    25%|      -0.8517109068|\n",
      "|    50%|       0.0931091041|\n",
      "|    75%|       1.1082230684|\n",
      "|    max|       4.4428105982|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                271|\n",
      "|   mean|0.08092355884391142|\n",
      "| stddev| 1.5280734856793479|\n",
      "|    min|      -3.9166166984|\n",
      "|    25%|      -0.8415096365|\n",
      "|    50%|       0.0991737429|\n",
      "|    75%|        1.052074211|\n",
      "|    max|       4.9927369335|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_standard.summary().show()\n",
    "test_df_standard.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[...|\n",
      "| 0.7645562902|(10,[0,3,4,5],[1....|\n",
      "|-1.1937191357|(10,[0,3,4,5],[1....|\n",
      "|-0.6283674193|(10,[3,4,5,7],[-1...|\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[...|\n",
      "| 0.3703458017|(10,[3,4,5],[-0.2...|\n",
      "| 1.0793990547|(10,[0,3,4,5,8],[...|\n",
      "| 1.5540425054|(10,[3,4,5,9],[-0...|\n",
      "| -3.820947183|(10,[2,3,4,5,8],[...|\n",
      "|-0.7602633557|(10,[2,3,4,5],[1....|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0...|\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[...|\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[...|\n",
      "| 0.2151428872|(10,[2,3,4,5],[1....|\n",
      "| 1.5208218686|(10,[0,3,4,5,6],[...|\n",
      "| 2.4562802894|(10,[0,3,4,5,6],[...|\n",
      "| 2.5008399099|(10,[3,4,5,6],[-0...|\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[...|\n",
      "| 0.6525371034|(10,[3,4,5,7],[-1...|\n",
      "| 2.6558999672|(10,[2,3,4,5,9],[...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_standard.show(10)\n",
    "test_df_standard.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "model = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='Label',\n",
    "    predictionCol='prediction',\n",
    "    maxIter=1500,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    tol=1e-06,\n",
    "    fitIntercept=True,\n",
    "    standardization=False,  # Training features already standardized\n",
    "    solver='auto',\n",
    "    loss='squaredError',\n",
    "    epsilon=1.35\n",
    ")\n",
    "\n",
    "model = model.fit(train_df_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------------------------------------------------------+--------------------+\n",
      "|Label        |features                                                                               |prediction          |\n",
      "+-------------+---------------------------------------------------------------------------------------+--------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[-0.8245169741090795,-0.5776631290602773,-1.7806332013886939,1.0])       |-1.2714231051846725 |\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[1.0,0.07039728984547404,1.131265633118539,1.4105667568225488,1.0])    |0.6947200744314178  |\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[1.0,0.39460778648915845,-0.6774042425435998,0.15481049086787357,1.0]) |0.1017908084729956  |\n",
      "|0.2151428872 |(10,[2,3,4,5],[1.0,0.6485196167049893,-1.2572306304001184,2.2172011038748094])         |-0.23909493880320543|\n",
      "|1.5208218686 |(10,[0,3,4,5,6],[1.0,0.3202815313099877,-0.3021079468194611,-0.1055637220781502,1.0])  |0.9988255340999144  |\n",
      "|2.4562802894 |(10,[0,3,4,5,6],[1.0,0.5395195955384848,-0.3107282772461097,0.9573723547538602,1.0])   |1.792902896013456   |\n",
      "|2.5008399099 |(10,[3,4,5,6],[-0.002156920804950496,-1.419674410280878,0.7099255315344997,1.0])       |2.307367536873264   |\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[1.0,-0.8336368832967098,0.035344317504657045,-1.3416805628269797,1.0])|0.11724271298122668 |\n",
      "|0.6525371034 |(10,[3,4,5,7],[-1.0165990675763117,-0.7712299433885768,0.22670887264950604,1.0])       |-0.2597913803510633 |\n",
      "|2.6558999672 |(10,[2,3,4,5,9],[1.0,2.8026925858148566,1.163990031070502,1.495267627338881,1.0])      |2.896131747234579   |\n",
      "+-------------+---------------------------------------------------------------------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.transform(test_df_standard)\n",
    "y_predict.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model trained on Standard Scaled data:\n",
      "\n",
      "MSE (Mean Squared Error) = 0.5094585827913202\n",
      "r2 (R-Squared metric (coefficient of determination)) = 0.7810091700181541\n"
     ]
    }
   ],
   "source": [
    "evaluator_mse = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "mse = evaluator_mse.evaluate(y_predict)\n",
    "r2 = evaluator_r2.evaluate(y_predict)\n",
    "\n",
    "print(\"Linear Regression model trained on Standard Scaled data:\\n\")\n",
    "print(f\"MSE (Mean Squared Error) = {mse}\")\n",
    "print(f\"r2 (R-Squared metric (coefficient of determination)) = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q7.2 Use from MinMaxScaler to scale the features in DF6 and split the DF6 into 30% testing and 70% training, then build linear regression model and calculate R2 and MSE.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Scaler requires the column be in vector format\n",
    "#  https://stackoverflow.com/a/60281624\n",
    "vector_assemblers = [\n",
    "    VectorAssembler(inputCols=[column], outputCol=column + \"_vector\")\n",
    "    for column in numerical_columns_to_scale\n",
    "]\n",
    "\n",
    "minmax_scalers = [\n",
    "    MinMaxScaler(inputCol=column + \"_vector\", outputCol=column + \"_scaled\")\n",
    "    for column in numerical_columns_to_scale\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=vector_assemblers + minmax_scalers)\n",
    "scaler_model = pipeline.fit(DF6)\n",
    "\n",
    "minmax_scaled_DF6 = scaler_model.transform(DF6)\n",
    "\n",
    "columns_to_drop = [col for col in numerical_columns_to_scale] + [f'{col}_vector' for col in numerical_columns_to_scale]\n",
    "minmax_scaled_DF6 = minmax_scaled_DF6.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "|        Label|F1_onehotencoded|F5_onehotencoded|           F2_scaled|           F3_scaled|           F4_scaled|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "|-0.8517109068|   (3,[0],[1.0])|   (4,[1],[1.0])| [0.509466521634977]|[0.4728396391216342]|[0.5525363678693873]|\n",
      "| 0.7645562902|   (3,[0],[1.0])|       (4,[],[])|[0.6552222847798332]|[0.4005191813664316]|[0.5251132546204497]|\n",
      "|-0.1682166727|   (3,[2],[1.0])|   (4,[3],[1.0])|[0.5304149103336981]|[0.7074910079944572]|[0.7247831160904461]|\n",
      "|-0.1504823794|   (3,[1],[1.0])|   (4,[2],[1.0])|[0.4321122994534692]|[0.39682336552437...|[0.6595872959632739]|\n",
      "| 2.5008399099|       (3,[],[])|   (4,[0],[1.0])|[0.5189813828992808]|[0.32111949946361...|[0.6050296023791764]|\n",
      "| 0.3703458017|       (3,[],[])|       (4,[],[])|[0.47801059061263...|[0.3695778491884058]| [0.488692447706386]|\n",
      "|-1.1937191357|   (3,[0],[1.0])|       (4,[],[])|[0.4359954956809631]|[0.7029982175967872]|[0.2398139340019353]|\n",
      "|-0.6283674193|       (3,[],[])|   (4,[1],[1.0])|[0.3436840504830293]|[0.40172637434103...|[0.22850103343021...|\n",
      "|-0.1172424373|   (3,[2],[1.0])|   (4,[3],[1.0])|[0.5815059438474088]|[0.4335455226420804]|[0.5101494065892802]|\n",
      "|-0.7615600037|       (3,[],[])|   (4,[1],[1.0])|[0.38938895613132...|[0.44865255135442...|[0.17934360417200...|\n",
      "+-------------+----------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minmax_scaled_DF6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Creating 'features' column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------------------------------------------------------------+\n",
      "|Label        |features                                                                            |\n",
      "+-------------+------------------------------------------------------------------------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[1.0,0.509466521634977,0.4728396391216342,0.5525363678693873,1.0])  |\n",
      "|0.7645562902 |(10,[0,3,4,5],[1.0,0.6552222847798332,0.4005191813664316,0.5251132546204497])       |\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[1.0,0.5304149103336981,0.7074910079944572,0.7247831160904461,1.0]) |\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[1.0,0.4321122994534692,0.39682336552437303,0.6595872959632739,1.0])|\n",
      "|2.5008399099 |(10,[3,4,5,6],[0.5189813828992808,0.32111949946361773,0.6050296023791764,1.0])      |\n",
      "|0.3703458017 |(10,[3,4,5],[0.47801059061263645,0.3695778491884058,0.488692447706386])             |\n",
      "|-1.1937191357|(10,[0,3,4,5],[1.0,0.4359954956809631,0.7029982175967872,0.2398139340019353])       |\n",
      "|-0.6283674193|(10,[3,4,5,7],[0.3436840504830293,0.40172637434103914,0.22850103343021744,1.0])     |\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[1.0,0.5815059438474088,0.4335455226420804,0.5101494065892802,1.0]) |\n",
      "|-0.7615600037|(10,[3,4,5,7],[0.38938895613132835,0.44865255135442206,0.17934360417200298,1.0])    |\n",
      "+-------------+------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\"F1_onehotencoded\", \"F2_scaled\", \"F3_scaled\", \"F4_scaled\", \"F5_onehotencoded\"]\n",
    "\n",
    "vector_assemblers = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector_assemblers])\n",
    "pipeline_model = pipeline.fit(minmax_scaled_DF6)\n",
    "\n",
    "minmax_scaled_DF6_vectorized = pipeline_model.transform(minmax_scaled_DF6)\n",
    "minmax_scaled_DF6_vectorized = minmax_scaled_DF6_vectorized.drop(*feature_columns)\n",
    "\n",
    "minmax_scaled_DF6_vectorized.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Splitting and fitting to LinearRegression model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a value for random seed value for reproducible train/test splits\n",
    "train_df_minmax, test_df_minmax = minmax_scaled_DF6_vectorized.randomSplit(weights=[0.7, 0.3], seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                745|\n",
      "|   mean|0.11241300524214759|\n",
      "| stddev| 1.4118029859923613|\n",
      "|    min|       -3.820947183|\n",
      "|    25%|      -0.8517109068|\n",
      "|    50%|       0.0931091041|\n",
      "|    75%|       1.1082230684|\n",
      "|    max|       4.4428105982|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|              Label|\n",
      "+-------+-------------------+\n",
      "|  count|                271|\n",
      "|   mean|0.08092355884391142|\n",
      "| stddev| 1.5280734856793479|\n",
      "|    min|      -3.9166166984|\n",
      "|    25%|      -0.8415096365|\n",
      "|    50%|       0.0991737429|\n",
      "|    75%|        1.052074211|\n",
      "|    max|       4.9927369335|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_minmax.summary().show()\n",
    "test_df_minmax.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.8517109068|(10,[0,3,4,5,7],[...|\n",
      "| 0.7645562902|(10,[0,3,4,5],[1....|\n",
      "|-1.1937191357|(10,[0,3,4,5],[1....|\n",
      "|-0.6283674193|(10,[3,4,5,7],[0....|\n",
      "|-0.1504823794|(10,[1,3,4,5,8],[...|\n",
      "| 0.3703458017|(10,[3,4,5],[0.47...|\n",
      "| 1.0793990547|(10,[0,3,4,5,8],[...|\n",
      "| 1.5540425054|(10,[3,4,5,9],[0....|\n",
      "| -3.820947183|(10,[2,3,4,5,8],[...|\n",
      "|-0.7602633557|(10,[2,3,4,5],[1....|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+--------------------+\n",
      "|        Label|            features|\n",
      "+-------------+--------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[0....|\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[...|\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[...|\n",
      "| 0.2151428872|(10,[2,3,4,5],[1....|\n",
      "| 1.5208218686|(10,[0,3,4,5,6],[...|\n",
      "| 2.4562802894|(10,[0,3,4,5,6],[...|\n",
      "| 2.5008399099|(10,[3,4,5,6],[0....|\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[...|\n",
      "| 0.6525371034|(10,[3,4,5,7],[0....|\n",
      "| 2.6558999672|(10,[2,3,4,5,9],[...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_minmax.show(10)\n",
    "test_df_minmax.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "model = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='Label',\n",
    "    predictionCol='prediction',\n",
    "    maxIter=1500,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    tol=1e-06,\n",
    "    fitIntercept=True,\n",
    "    standardization=False,  # We are testing MinMax scaling vs Standard Scaling\n",
    "    solver='auto',\n",
    "    loss='squaredError',\n",
    "    epsilon=1.35\n",
    ")\n",
    "\n",
    "model = model.fit(train_df_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------------------------------------------------------------+--------------------+\n",
      "|Label        |features                                                                            |prediction          |\n",
      "+-------------+------------------------------------------------------------------------------------+--------------------+\n",
      "|-0.7615600037|(10,[3,4,5,7],[0.38938895613132835,0.44865255135442206,0.17934360417200298,1.0])    |-1.27142310518466   |\n",
      "|-0.1682166727|(10,[2,3,4,5,9],[1.0,0.5304149103336981,0.7074910079944572,0.7247831160904461,1.0]) |0.6947200744314164  |\n",
      "|-0.1172424373|(10,[2,3,4,5,9],[1.0,0.5815059438474088,0.4335455226420804,0.5101494065892802,1.0]) |0.10179080847300037 |\n",
      "|0.2151428872 |(10,[2,3,4,5],[1.0,0.6215188921059652,0.345723624694386,0.8626529613882171])        |-0.23909493880320332|\n",
      "|1.5208218686 |(10,[0,3,4,5,6],[1.0,0.5697931670137795,0.49038880137776464,0.4656462775202283,1.0])|0.9988255340999079  |\n",
      "|2.4562802894 |(10,[0,3,4,5,6],[1.0,0.6043420153790731,0.4890831454178778,0.6473231835555155,1.0]) |1.7929028960134432  |\n",
      "|2.5008399099 |(10,[3,4,5,6],[0.5189813828992808,0.32111949946361773,0.6050296023791764,1.0])      |2.3073675368732567  |\n",
      "|-0.0508083621|(10,[0,3,4,5,9],[1.0,0.38795178617301174,0.5415001321836068,0.2543693362062366,1.0])|0.11724271298123634 |\n",
      "|0.6525371034 |(10,[3,4,5,7],[0.35911950842278134,0.4193344565418644,0.5224382693306426,1.0])      |-0.25979138035105764|\n",
      "|2.6558999672 |(10,[2,3,4,5,9],[1.0,0.9609863815872802,0.7124475239404825,0.7392601786620411,1.0]) |2.8961317472345516  |\n",
      "+-------------+------------------------------------------------------------------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.transform(test_df_minmax)\n",
    "y_predict.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model trained on Min-Max Scaled data:\n",
      "\n",
      "MSE (Mean Squared Error) = 0.509458582791323\n",
      "r2 (R-Squared metric (coefficient of determination)) = 0.7810091700181528\n"
     ]
    }
   ],
   "source": [
    "evaluator_mse = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "mse = evaluator_mse.evaluate(y_predict)\n",
    "r2 = evaluator_r2.evaluate(y_predict)\n",
    "\n",
    "print(\"Linear Regression model trained on Min-Max Scaled data:\\n\")\n",
    "print(f\"MSE (Mean Squared Error) = {mse}\")\n",
    "print(f\"r2 (R-Squared metric (coefficient of determination)) = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __Q7.3 Based on R2 and MSE, which model perform better? Justify your answer.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q7.3 Answer:__\n",
    "The two models are showing the exact same predictions, and MSE and R-Squared values, I was not able to find the reason why this is occuring."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
