{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ChurnRateApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043\n",
      "7032\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "churnSchema = StructType([\n",
    "    StructField(\"customerID\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"SeniorCitizen\", IntegerType(), True),\n",
    "    StructField(\"Partner\", StringType(), True),\n",
    "    StructField(\"Dependents\", StringType(), True),\n",
    "    StructField(\"tenure\", IntegerType(), True),\n",
    "    StructField(\"PhoneService\", StringType(), True),\n",
    "    StructField(\"MultipleLines\", StringType(), True),\n",
    "    StructField(\"InternetService\", StringType(), True),\n",
    "    StructField(\"OnlineSecurity\", StringType(), True),\n",
    "    StructField(\"OnlineBackup\", StringType(), True),\n",
    "    StructField(\"DeviceProtection\", StringType(), True),\n",
    "    StructField(\"TechSupport\", StringType(), True),\n",
    "    StructField(\"StreamingTV\", StringType(), True),\n",
    "    StructField(\"StreamingMovies\", StringType(), True),\n",
    "    StructField(\"Contract\", StringType(), True),\n",
    "    StructField(\"PaperlessBilling\", StringType(), True),\n",
    "    StructField(\"PaymentMethod\", StringType(), True),\n",
    "    StructField(\"MonthlyCharges\", FloatType(), True),\n",
    "    StructField(\"TotalCharges\", FloatType(), True),\n",
    "    StructField(\"Churn\", StringType(), True),\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "churn_df = spark.read.csv(\"../data_sets/churn_rate_data.csv\", header=True, schema=churnSchema)\n",
    "\n",
    "#the data only has 11 nulls. consediring the small number these records will just be dropped.\n",
    "print(churn_df.count())\n",
    "print(churn_df.dropna().count())\n",
    "\n",
    "churn_df = churn_df.dropna()\n",
    "\n",
    "# change the label column to label\n",
    "churn_df = churn_df.withColumnRenamed('Churn','label')\n",
    "\n",
    "# List of columns need to be indexed and featurized\n",
    "col_list = [\"gender\", \"SeniorCitizen\", \n",
    "              \"Partner\", \"Dependents\",\n",
    "              \"PhoneService\", \n",
    "              \"MultipleLines\", \"InternetService\", \n",
    "              \"OnlineSecurity\", \"OnlineBackup\",\n",
    "              \"DeviceProtection\", \"TechSupport\", \n",
    "              \"StreamingTV\", \"StreamingMovies\",\n",
    "              \"Contract\", \"PaperlessBilling\", \n",
    "              \"PaymentMethod\"]\n",
    "numerical_cols = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "featurized_col_list = col_list + numerical_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>labelIndex</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 29.850000381469727, 29.850000381469727, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(34.0, 56.95000076293945, 1889.5, 1.0, 0.0, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(2.0, 53.849998474121094, 108.1500015258789, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(45.0, 42.29999923706055, 1840.75, 1.0, 0.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(2.0, 70.69999694824219, 151.64999389648438, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(8.0, 99.6500015258789, 820.5, 0.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(22.0, 89.0999984741211, 1949.4000244140625, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(10.0, 29.75, 301.8999938964844, 0.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(28.0, 104.80000305175781, 3046.050048828125, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(62.0, 56.150001525878906, 3487.949951171875, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  labelIndex                                           features\n",
       "0    No         0.0  (1.0, 29.850000381469727, 29.850000381469727, ...\n",
       "1    No         0.0  (34.0, 56.95000076293945, 1889.5, 1.0, 0.0, 1....\n",
       "2   Yes         1.0  (2.0, 53.849998474121094, 108.1500015258789, 1...\n",
       "3    No         0.0  (45.0, 42.29999923706055, 1840.75, 1.0, 0.0, 1...\n",
       "4   Yes         1.0  (2.0, 70.69999694824219, 151.64999389648438, 0...\n",
       "5   Yes         1.0  (8.0, 99.6500015258789, 820.5, 0.0, 1.0, 1.0, ...\n",
       "6    No         0.0  (22.0, 89.0999984741211, 1949.4000244140625, 1...\n",
       "7    No         0.0  (10.0, 29.75, 301.8999938964844, 0.0, 1.0, 1.0...\n",
       "8   Yes         1.0  (28.0, 104.80000305175781, 3046.050048828125, ...\n",
       "9    No         0.0  (62.0, 56.150001525878906, 3487.949951171875, ..."
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List of features and lable indexed \n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f'{c}_indexed')\n",
    "    for c in col_list\n",
    "    ]\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "indexers.append(label_indexer)\n",
    "\n",
    "#one hot encode the numeric\n",
    "encoder = OneHotEncoder(inputCols = [f'{c}_indexed' for c in col_list], \n",
    "                        outputCols=[f'{c}_vector' for c in col_list])\n",
    "encoder.setDropLast(False)\n",
    "\n",
    "# Vectorizing the features\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols= numerical_cols + [f'{c}_vector' for c in col_list], outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "# Define the pipline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [encoder, vector_assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(churn_df)\n",
    "tranformed_df = pipeline_model.transform(churn_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_drop = [f'{c}_indexed' for c in col_list] + [f'{c}_vector' for c in col_list] \\\n",
    "            + [f'{c}' for c in featurized_col_list] + ['customerID']\n",
    "transformed_df = tranformed_df.drop(*cols_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Scaling\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(tranformed_df)\n",
    "\n",
    "scaled_df = scalerModel.transform(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality reduction\n",
    "#Still needs more work \n",
    "\n",
    "pca = PCA(k=45, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaled_df)\n",
    "reduced_df = model.transform(scaled_df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"pcaFeatures\", outputCol=\"scaledPcaFeatures\")\n",
    "scalerModel = scaler.fit(reduced_df)\n",
    "\n",
    "scaled_pca_df = scalerModel.transform(reduced_df)\n",
    "\n",
    "\n",
    "pca = PCA(k=45, inputCol=\"scaledFeatures\", outputCol=\"pcaScaledFeatures\")\n",
    "model = pca.fit(scaled_pca_df)\n",
    "pca_scaled_df = model.transform(scaled_pca_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 11\n",
    "(training,testing) = pca_scaled_df.randomSplit([0.8,0.2], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.208453 \n",
      "Accuracy = 0.791547 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# train our model using training data\n",
    "dt = DecisionTreeClassifier(labelCol=\"labelIndex\", featuresCol=\"features\")\n",
    "model = dt.fit(training)\n",
    "\n",
    "# test our model and make predictions using testing data\n",
    "predictions = model.transform(testing)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.199857 \n",
      "Accuracy = 0.800143 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rd = RandomForestClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", maxDepth = 30)\n",
    "model = rd.fit(training)\n",
    "predictions = model.transform(testing)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.270057 \n",
      "Accuracy = 0.729943 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"labelIndex\", featuresCol=\"scaledFeatures\", maxIter=30, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "predictions = lrModel.transform(testing)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.198272\n",
      "Accuracy = 0.801728 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "(training,testing) = pca_scaled_df.randomSplit([0.7,0.3], seed=seed)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"labelIndex\", featuresCol=\"scaledFeatures\", maxIter=30)\n",
    "\n",
    "model = gbt.fit(training)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testing)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.235718\n",
      "Accuracy = 0.764282 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "(training,testing) = pca_scaled_df.randomSplit([0.7,0.3], seed=seed)\n",
    "lsvc = LinearSVC(labelCol=\"labelIndex\", featuresCol=\"scaledFeatures\", maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "predictions = lsvcModel.transform(testing)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
